{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "491ad4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dropout, Flatten, Dense,Conv2DTranspose,UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "tf.config.run_functions_eagerly(True)\n",
    "#from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dropout, Flatten, Dense,Conv2DTranspose,UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7532be28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14320\n",
      "Found 14320 files belonging to 12 classes.\n",
      "Using 11456 files for training.\n",
      "Found 14320 files belonging to 12 classes.\n",
      "Using 2864 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# training without normalizing the image (scaling)\n",
    "dataset_dir = Path('C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset')\n",
    "image_count = len(list(dataset_dir.glob('*/*.png')))\n",
    "print(image_count)\n",
    "#dataset_dir = 'C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset'\n",
    "\n",
    "checkpoint_path = 'C:/deva/Miniproject/automation-using-hand-gestures-master/TrainedModel/Gesture12RecognitionModelBest.keras'\n",
    "best_checkpoint_path = 'C:/deva/Miniproject/automation-using-hand-gestures-master/TrainedModel/Gesture12RecognitionModelBest.keras'\n",
    "saved_model_path = 'C:/deva/Miniproject/automation-using-hand-gestures-master/TrainedModel/Gesture12RecognitionModelBest.keras'\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 100\n",
    "img_width = 89\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  dataset_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "#test_ratio = 0.2\n",
    "#validation_ratio = 0.2\n",
    "#batch_size = 50\n",
    "#epochs = 10\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  dataset_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Direction_left', 'Direction_right', 'Fist', 'Five-palm', 'Four', 'OK', 'One', 'Stop', 'Three', 'Thumbs_down', 'Thumbs_up', 'Two']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 89, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")\n",
    "num_classes = len(class_names)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=[ 100, 89, 3], name='input')\n",
    "convnet = Conv2D(32,2, activation='relu')(input_layer)\n",
    "convnet =MaxPooling2D( 2,2)(convnet)\n",
    "convnet =Conv2D( 64,( 2,2), activation='relu')(convnet)\n",
    "convnet = MaxPooling2D( 2,2)(convnet)\n",
    "convnet =Conv2D( 128, ( 2,2), activation='relu')(convnet)\n",
    "convnet = MaxPooling2D( 2,2)(convnet)\n",
    "\n",
    "convnet =Conv2D( 256, ( 2,2), activation='relu')(convnet)\n",
    "convnet = MaxPooling2D( 2,2)(convnet)\n",
    "\n",
    "convnet = Flatten()(convnet) \n",
    "convnet = Dense(128,activation='relu')(convnet)\n",
    "convnet=Dropout(0.50)(convnet)\n",
    "convnet=Dense(12,activation='softmax')(convnet)\n",
    "model = Model(inputs=input_layer, outputs=convnet)\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.001), loss='SparseCategoricalCrossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.save('C:/deva/Miniproject/automation-using-hand-gestures-master/TrainedModel/Gesture12RecognitionModelBest.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/10\n",
      "358/358 [==============================] - 249s 673ms/step - loss: 1.2201 - accuracy: 0.8974 - val_loss: 0.0125 - val_accuracy: 0.9962\n",
      "Epoch 2/10\n",
      "358/358 [==============================] - 131s 367ms/step - loss: 0.0619 - accuracy: 0.9849 - val_loss: 0.0557 - val_accuracy: 0.9850\n",
      "Epoch 3/10\n",
      "358/358 [==============================] - 94s 264ms/step - loss: 0.0392 - accuracy: 0.9907 - val_loss: 0.0016 - val_accuracy: 0.9993\n",
      "Epoch 4/10\n",
      "358/358 [==============================] - 121s 337ms/step - loss: 0.0275 - accuracy: 0.9935 - val_loss: 0.0024 - val_accuracy: 0.9993\n",
      "Epoch 5/10\n",
      "358/358 [==============================] - 218s 610ms/step - loss: 0.0294 - accuracy: 0.9948 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
      "Epoch 6/10\n",
      "358/358 [==============================] - 217s 605ms/step - loss: 0.0190 - accuracy: 0.9966 - val_loss: 5.9039e-07 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "358/358 [==============================] - 215s 600ms/step - loss: 0.0169 - accuracy: 0.9968 - val_loss: 8.2783e-06 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "358/358 [==============================] - 219s 611ms/step - loss: 0.0190 - accuracy: 0.9960 - val_loss: 1.4660e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "358/358 [==============================] - 167s 467ms/step - loss: 0.0171 - accuracy: 0.9972 - val_loss: 0.0162 - val_accuracy: 0.9990\n",
      "Epoch 10/10\n",
      "358/358 [==============================] - 89s 250ms/step - loss: 0.0229 - accuracy: 0.9969 - val_loss: 0.0013 - val_accuracy: 0.9997\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "snapshot_step = 100\n",
    "checkpoint = ModelCheckpoint(filepath='C:/deva/Miniproject/automation-using-hand-gestures-master/TrainedModel/Gesture12RecognitionModel_{epoch:02d}.keras', save_best_only=True, period=snapshot_step)\n",
    "model.fit(train_ds, epochs=10,\n",
    "           validation_data=val_ds,\n",
    "        callbacks=[checkpoint])\n",
    "\n",
    "model.save('C:/deva/Miniproject/automation-using-hand-gestures-master/TrainedModel/Gesture12RecognitionModelBest.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 100, 89, 3)]      0         \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 99, 88, 32)        416       \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPooli  (None, 49, 44, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 48, 43, 64)        8256      \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPooli  (None, 24, 21, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 23, 20, 128)       32896     \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPooli  (None, 11, 10, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 10, 9, 256)        131328    \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPooli  (None, 5, 4, 256)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 5120)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               655488    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 12)                1548      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 829932 (3.17 MB)\n",
      "Trainable params: 829932 (3.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (32, 1) and (32, 12) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\deva\\Miniproject\\automation-using-hand-gestures-master\\ModelTrainer.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/deva/Miniproject/automation-using-hand-gestures-master/ModelTrainer.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/deva/Miniproject/automation-using-hand-gestures-master/ModelTrainer.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/deva/Miniproject/automation-using-hand-gestures-master/ModelTrainer.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   train_ds,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/deva/Miniproject/automation-using-hand-gestures-master/ModelTrainer.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   validation_data\u001b[39m=\u001b[39;49mval_ds,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/deva/Miniproject/automation-using-hand-gestures-master/ModelTrainer.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/deva/Miniproject/automation-using-hand-gestures-master/ModelTrainer.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:5575\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m   5573\u001b[0m target \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(target)\n\u001b[0;32m   5574\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(output)\n\u001b[1;32m-> 5575\u001b[0m target\u001b[39m.\u001b[39;49mshape\u001b[39m.\u001b[39;49massert_is_compatible_with(output\u001b[39m.\u001b[39;49mshape)\n\u001b[0;32m   5577\u001b[0m output, from_logits \u001b[39m=\u001b[39m _get_logits(\n\u001b[0;32m   5578\u001b[0m     output, from_logits, \u001b[39m\"\u001b[39m\u001b[39mSoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   5579\u001b[0m )\n\u001b[0;32m   5580\u001b[0m \u001b[39mif\u001b[39;00m from_logits:\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (32, 1) and (32, 12) are incompatible"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step\n",
      "This image most likely belongs to Five-palm with a 19.82 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "sunflower_url =  Path('d:/movies/30_l=3.png')\n",
    "\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    sunflower_url, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e89f383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset/Direction_left done.\n",
      "C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset/Direction_right done.\n",
      "C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset/Fist done.\n",
      "C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset/Five-palm done.\n",
      "C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset/Four done.\n",
      "C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset/OK done.\n",
      "C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset/One done.\n",
      "C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset/Stop done.\n",
      "C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset/Three done.\n",
      "C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset/Thumbs_down done.\n",
      "C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset/Thumbs_up done.\n",
      "C:/deva/Miniproject/automation-using-hand-gestures-master/Dataset/Two done.\n"
     ]
    }
   ],
   "source": [
    "for d in alldirs:\n",
    "    for img in os.listdir(d):\n",
    "        label = re.findall('[l]=[0-9]+', img)\n",
    "        label = label[0][2:]\n",
    "        \n",
    "        current_img = f\"{d}/{img}\"\n",
    "        image = cv2.imread(current_img)\n",
    "        resized_image = cv2.resize(image, (100, 89))\n",
    "        gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        all_images.append(gray_image.reshape(89, 100, 1))\n",
    "        class_ids.append(label)\n",
    "    print(f\"{d} done.\")\n",
    "            \n",
    "all_images = np.array(all_images)\n",
    "class_ids = np.array(class_ids)\n",
    "\n",
    "#print(len(all_images))\n",
    "#print(len(class_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "882b58d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1472, 89, 100, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(all_images, class_ids, test_size=test_ratio)\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=validation_ratio)\n",
    "\n",
    "x_train.shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "287ef368",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)\n",
    "y_validation = to_categorical(y_validation, n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1dfb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "#                           [batch, height, width, in_channels]\n",
    "#input_layer = Input(shape=(89, 100, 1), name='input')\n",
    "input_layer = Input(shape=[ 89, 100, 1], name='input')\n",
    "convnet = Conv2D(32,2, activation='relu')(input_layer)\n",
    "convnet =MaxPooling2D( 2,2)(convnet)\n",
    "convnet =Conv2D( 64,( 2,2), activation='relu')(convnet)\n",
    "convnet = MaxPooling2D( 2,2)(convnet)\n",
    "convnet =Conv2D( 128, ( 2,2), activation='relu')(convnet)\n",
    "convnet = MaxPooling2D( 2,2)(convnet)\n",
    "\n",
    "convnet =Conv2D( 256, ( 2,2), activation='relu')(convnet)\n",
    "convnet = MaxPooling2D( 2,2)(convnet)\n",
    "convnet = Conv2DTranspose( 256, (2,2), activation='relu')(convnet)\n",
    "convnet = UpSampling2D(  (2,2))(convnet)\n",
    "\n",
    "convnet = Conv2DTranspose( 128, (2,2), activation='relu')(convnet)\n",
    "convnet = UpSampling2D( (2,2))(convnet)\n",
    "\n",
    "convnet = Conv2DTranspose( 64,  (2,2), activation='relu')(convnet)\n",
    "convnet = UpSampling2D( (2,2))(convnet)\n",
    "convnet = Flatten()(convnet) \n",
    "convnet = Dense(1000,activation='relu')(convnet)\n",
    "convnet=Dropout(0.75)(convnet)\n",
    "convnet=Dense(12,activation='softmax')(convnet)\n",
    "#model = Sequential() \n",
    "#model.add(Conv2D(32, (3,3),padding='same' ,activation='relu',input_shape=input_shape))\n",
    "#model.add(MaxPooling2D((2,2),padding='same'))\n",
    "#model.add(Conv2D(64, (3,3)   ,padding='same' ,activation='relu'))\n",
    "#model.add(MaxPooling2D((2,2),padding='same'))\n",
    "#model.add(Conv2D(128, (3,3)   ,padding='same' ,activation='relu'))\n",
    "#model.add(MaxPooling2D((2,2),padding='same'))\n",
    "###model.add(Conv2D(256, (3,3)   ,padding='same' ,activation='relu'))\n",
    "#model.add(MaxPooling2D((2,2),padding='same'))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dropout(0.75))\n",
    "#   model.add( Dense(12, activation='softmax'))\n",
    "model = Model(inputs=input_layer, outputs=convnet)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.save('C:/deva/Miniproject/automation-using-hand-gestures-master/TrainedModel/Gesture12RecognitionModelBest.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba92e50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/10\n",
      "46/46 [==============================] - 121s 3s/step - loss: 7.9219 - accuracy: 0.5577 - val_loss: 0.3385 - val_accuracy: 0.9130\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 119s 3s/step - loss: 0.2275 - accuracy: 0.9348 - val_loss: 0.0478 - val_accuracy: 0.9918\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 118s 3s/step - loss: 0.0432 - accuracy: 0.9851 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 116s 3s/step - loss: 0.0265 - accuracy: 0.9925 - val_loss: 0.0186 - val_accuracy: 0.9946\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 115s 2s/step - loss: 0.0569 - accuracy: 0.9837 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 113s 2s/step - loss: 0.0505 - accuracy: 0.9830 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 113s 2s/step - loss: 0.0816 - accuracy: 0.9776 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 113s 2s/step - loss: 0.0356 - accuracy: 0.9905 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 112s 2s/step - loss: 0.0238 - accuracy: 0.9932 - val_loss: 3.6367e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 112s 2s/step - loss: 0.0455 - accuracy: 0.9871 - val_loss: 4.7293e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "snapshot_step = 100\n",
    "checkpoint = ModelCheckpoint(filepath='C:/deva/Miniproject/automation-using-hand-gestures-master/TrainedModel/Gesture12RecognitionModel_{epoch:02d}.keras', save_best_only=True, period=snapshot_step)\n",
    "model.fit(x_train, y_train, epochs=10,\n",
    "           validation_data=(x_validation, y_validation),\n",
    "        callbacks=[checkpoint])\n",
    "\n",
    "model.save('C:/deva/Miniproject/automation-using-hand-gestures-master/TrainedModel/Gesture12RecognitionModelBest.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e121e8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 89, 100, 1)]      0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 88, 99, 32)        160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 44, 49, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 43, 48, 64)        8256      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 21, 24, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 20, 23, 128)       32896     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 10, 11, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 9, 10, 256)        131328    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 4, 5, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 5, 6, 256)         262400    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2  (None, 10, 12, 256)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 11, 13, 128)       131200    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSamplin  (None, 22, 26, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 23, 27, 64)        32832     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSamplin  (None, 46, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 158976)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              158977000 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                12012     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 159588084 (608.78 MB)\n",
      "Trainable params: 159588084 (608.78 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "15/15 [==============================] - 2s 157ms/step - loss: 8.6987e-04 - accuracy: 1.0000\n",
      "[0.0008698708261363208, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "This image most likely belongs to Thumbs_down with a 19.82 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "sunflower_url =  Path('D:/movies/1_l=1.png')\n",
    "\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    sunflower_url, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
